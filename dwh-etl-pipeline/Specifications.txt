Objective: Build an ETL pipeline that extracts data from various sources and stores it in a data lake
------

Data Selection of Choice: EPL Leauge

Core Parts:

Data sources
    - Scope
        - API, Database, CSV

    - Interfaces to be able to dynamically select which API, CSV, or Database to pull from

Aggregation and Cleaning Pipeline

    - Cleaning
        - Pandas,Data validation here
    - Deduplication
        - Pandas
    - Aggregation
        - Pandas
    - Translation to a suitable format

Batch Processing

    - Time-based scheduling
    - Data size-based scheduling
    *Cron for Mac*

Data Lake (Depositing Data), should learn how to use Snowflake here or an OLTP Database

---------

Architecture

Data Sources --> Aggregation/Cleaning/Batch Processing --> AWS S3 --> AWS Glue Crawler --> Amazon Athena
